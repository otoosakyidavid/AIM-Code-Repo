{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "SMS_spam_classifier- using NLP.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1oboG2WIgm5"
      },
      "source": [
        "## **In this practice we will use natural language processing to build a spam classifier**\n",
        "\n",
        "## **We will make use of the sms spam classification data for the implementation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QT0e91prLONA"
      },
      "source": [
        "**Data processing**\n",
        "*   Import the required packages\n",
        "*   Load the data into train and test variables\n",
        "*   Remove the unwanted data columns\n",
        "*   Build wordcloud to see which message is spam and which is not.\n",
        "*   Remove the stop words and punctuations\n",
        "*   Convert the text data into vectors\n",
        "\n",
        "**Building a classification model**\n",
        "*   Split the data into train and test sets\n",
        "*   Use Sklearn built in classifiers to build the models\n",
        "*   Train the data on the model\n",
        "*   Make predictions on new data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "-izgLLGNT1dJ"
      },
      "source": [
        "### **Installing Dependencies**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "xCawppf-T1dL",
        "outputId": "9683053c-3df4-4190-c8fe-f898603fdaa9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "#Install Packages\n",
        "!pip install wordcloud"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.6/dist-packages (1.5.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from wordcloud) (7.0.0)\n",
            "Requirement already satisfied: numpy>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from wordcloud) (1.18.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ag7hrorT1dS"
      },
      "source": [
        "## **Import the required packages that is to be used**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWibLvVTT1dS"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import csv\n",
        "import sklearn\n",
        "import pickle\n",
        "from wordcloud import WordCloud\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.tree import DecisionTreeClassifier \n",
        "from sklearn.model_selection import GridSearchCV,train_test_split,StratifiedKFold,cross_val_score,learning_curve\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vS4d4NtJT1dW"
      },
      "source": [
        "## **Preprocessing and Exploring the Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcTtyR6vT1dY",
        "outputId": "e0f41928-e2d5-44c2-815a-78fa28b70e37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "data = pd.read_csv('https://raw.githubusercontent.com/akshaybhatia10/SMS-Spam-Classification/master/data/spam.csv', encoding='latin-1')\n",
        "data.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>v1</th>\n",
              "      <th>v2</th>\n",
              "      <th>Unnamed: 2</th>\n",
              "      <th>Unnamed: 3</th>\n",
              "      <th>Unnamed: 4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     v1  ... Unnamed: 4\n",
              "0   ham  ...        NaN\n",
              "1   ham  ...        NaN\n",
              "2  spam  ...        NaN\n",
              "3   ham  ...        NaN\n",
              "4   ham  ...        NaN\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OHtOc2jT1dc"
      },
      "source": [
        "## **Removing unwanted columns**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sclvYeoVT1dd"
      },
      "source": [
        "data = data.drop([\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis=1)\n",
        "data = data.rename(columns={\"v2\" : \"text\", \"v1\":\"label\"})"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPCJVr9zT1di",
        "outputId": "ed4a1a1e-6fd9-482f-8946-dcb69be21141",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        }
      },
      "source": [
        "data[1990:2000]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1990</th>\n",
              "      <td>ham</td>\n",
              "      <td>HI DARLIN IVE JUST GOT BACK AND I HAD A REALLY...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1991</th>\n",
              "      <td>ham</td>\n",
              "      <td>No other Valentines huh? The proof is on your ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1992</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free tones Hope you enjoyed your new content. ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1993</th>\n",
              "      <td>ham</td>\n",
              "      <td>Eh den sat u book e kb liao huh...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1994</th>\n",
              "      <td>ham</td>\n",
              "      <td>Have you been practising your curtsey?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1995</th>\n",
              "      <td>ham</td>\n",
              "      <td>Shall i come to get pickle</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1996</th>\n",
              "      <td>ham</td>\n",
              "      <td>Lol boo I was hoping for a laugh</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1997</th>\n",
              "      <td>ham</td>\n",
              "      <td>\\YEH I AM DEF UP4 SOMETHING SAT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998</th>\n",
              "      <td>ham</td>\n",
              "      <td>Well, I have to leave for my class babe ... Yo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1999</th>\n",
              "      <td>ham</td>\n",
              "      <td>LMAO where's your fish memory when I need it?</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     label                                               text\n",
              "1990   ham  HI DARLIN IVE JUST GOT BACK AND I HAD A REALLY...\n",
              "1991   ham  No other Valentines huh? The proof is on your ...\n",
              "1992  spam  Free tones Hope you enjoyed your new content. ...\n",
              "1993   ham                 Eh den sat u book e kb liao huh...\n",
              "1994   ham             Have you been practising your curtsey?\n",
              "1995   ham                         Shall i come to get pickle\n",
              "1996   ham                   Lol boo I was hoping for a laugh\n",
              "1997   ham                    \\YEH I AM DEF UP4 SOMETHING SAT\n",
              "1998   ham  Well, I have to leave for my class babe ... Yo...\n",
              "1999   ham      LMAO where's your fish memory when I need it?"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "zNWfIdYvT1dm",
        "outputId": "ff1affde-1ba2-4586-8356-4be806be30b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "data['label'].value_counts()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ham     4825\n",
              "spam     747\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUaVXJY9T1d1"
      },
      "source": [
        "# Import nltk packages and Punkt Tokenizer Models\n",
        "import nltk\n",
        "nltk.download(\"punkt\")\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2Fi_co9T1d4"
      },
      "source": [
        "### **WordClouds- to see which words are common in SPAM and NOT SPAM mesaages**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cbnNrgnT1d5"
      },
      "source": [
        "ham_words = ''\n",
        "spam_words = ''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xtSTyueT1d8"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o13b2oOET1eB"
      },
      "source": [
        "# Creating a corpus of spam messages\n",
        "for val in data[data['label'] == 'spam'].text:\n",
        "    text = val.lower()\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    for words in tokens:\n",
        "        spam_words = spam_words + words + ' '\n",
        "# Creating a corpus of ham messages        \n",
        "for val in data[data['label'] == 'ham'].text:\n",
        "    text = val.lower()\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    for words in tokens:\n",
        "        ham_words = ham_words + words + ' '"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wripxz0xT1eF"
      },
      "source": [
        "## **Creating  Spam wordcloud and ham wordcloud**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zv7Fryv5T1eG"
      },
      "source": [
        "spam_wordcloud = WordCloud(width=500, height=300).generate(spam_words)\n",
        "ham_wordcloud = WordCloud(width=500, height=300).generate(ham_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIHFH-BJT1eJ"
      },
      "source": [
        "#Spam Word cloud\n",
        "plt.figure( figsize=(10,8), facecolor='w')\n",
        "plt.imshow(spam_wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.tight_layout(pad=0)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHgEfutRT1eO"
      },
      "source": [
        "#Creating Ham wordcloud\n",
        "plt.figure( figsize=(10,8), facecolor='g')\n",
        "plt.imshow(ham_wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.tight_layout(pad=0)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opaPXshjT1eR"
      },
      "source": [
        "data = data.replace(['ham','spam'],[0, 1]) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRveTZ9GT1eV"
      },
      "source": [
        "data.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5Lz8iQAT1eY"
      },
      "source": [
        "## **Removing Stopwords from the messages**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GO4u5sUqT1eY"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FMXzJoeT1eb"
      },
      "source": [
        "## **Remove punctuation  and stopwords**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edDpGjhST1ec"
      },
      "source": [
        "#remove the punctuations and stopwords\n",
        "import string\n",
        "def text_process(text):\n",
        "    \n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    text = [word for word in text.split() if word.lower() not in stopwords.words('english')]\n",
        "    \n",
        "    return \" \".join(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUqNFnNPT1ef"
      },
      "source": [
        "data['text'] = data['text'].apply(text_process)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEsCTryOT1ek"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0ZAwZiAT1en",
        "outputId": "b8d9c95c-7469-4619-e865-c411f5e1e970",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "text = pd.DataFrame(data['text'])\n",
        "label = pd.DataFrame(data['label'])\n",
        "label"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5567</th>\n",
              "      <td>spam</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5568</th>\n",
              "      <td>ham</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5569</th>\n",
              "      <td>ham</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5570</th>\n",
              "      <td>ham</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5571</th>\n",
              "      <td>ham</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5572 rows Ã— 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     label\n",
              "0      ham\n",
              "1      ham\n",
              "2     spam\n",
              "3      ham\n",
              "4      ham\n",
              "...    ...\n",
              "5567  spam\n",
              "5568   ham\n",
              "5569   ham\n",
              "5570   ham\n",
              "5571   ham\n",
              "\n",
              "[5572 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkDiOQ7TT1er"
      },
      "source": [
        "## **Converting words to vectors**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLk0XaOCT1es",
        "outputId": "5a53d429-d513-4077-89a2-6f8d7ed24fe6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "## Counting how many times a word appears in the dataset\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "total_counts = Counter()\n",
        "for i in range(len(text)):\n",
        "    for word in text.values[i][0].split(\" \"):\n",
        "        total_counts[word] += 1\n",
        "\n",
        "print(\"Total words in data set: \", len(total_counts))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total words in data set:  15586\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGioZnysT1e7",
        "outputId": "53dc4bfe-240e-427a-b80a-5ea668b9d9c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Sorting in decreasing order (Word with highest frequency appears first)\n",
        "vocab = sorted(total_counts, key=total_counts.get, reverse=True)\n",
        "print(vocab[:60])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['to', 'you', 'I', 'a', 'the', 'and', 'in', 'is', 'i', 'u', 'for', 'my', '', 'of', 'your', 'me', 'on', 'have', '2', 'that', 'it', 'are', 'call', 'or', 'be', 'at', 'with', 'not', 'will', 'get', 'can', 'U', 'so', 'ur', \"I'm\", 'but', '&lt;#&gt;', 'You', 'from', '4', 'do', 'up', 'just', 'if', '.', 'go', 'when', 'know', 'this', 'like', 'we', 'all', 'out', 'got', 'was', 'come', 'now', '?', 'am', '...']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pme-lXRfT1e_"
      },
      "source": [
        "# Mapping from words to index\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "word2idx = {}\n",
        "#print vocab_size\n",
        "for i, word in enumerate(vocab):\n",
        "    word2idx[word] = i"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvKOwVGBT1fF"
      },
      "source": [
        "# Text to Vector\n",
        "def text_to_vector(text):\n",
        "    word_vector = np.zeros(vocab_size)\n",
        "    for word in text.split(\" \"):\n",
        "        if word2idx.get(word) is None:\n",
        "            continue\n",
        "        else:\n",
        "            word_vector[word2idx.get(word)] += 1\n",
        "    return np.array(word_vector)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlQa0HhQT1fJ"
      },
      "source": [
        "# Convert all titles to vectors\n",
        "word_vectors = np.zeros((len(text), len(vocab)), dtype=np.int_)\n",
        "for i, (_, text_) in enumerate(text.iterrows()):\n",
        "    word_vectors[i] = text_to_vector(text_[0])"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cR-AVicuT1fS"
      },
      "source": [
        "## **Converting words to vectors using TFIDF Vectorizer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1p_1l3LrVsaB",
        "outputId": "1e1232dc-60b4-4af5-b2e3-31fe5917e39b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#convert the text data into vectors\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "vectors = vectorizer.fit_transform(data['text'])\n",
        "vectors.shape"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5572, 8672)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9fGfx5sT1fX"
      },
      "source": [
        "#features = word_vectors\n",
        "features = vectors"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lN_sj7EhT1fb"
      },
      "source": [
        "## **Splitting into training and test set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKy_fPvST1fc"
      },
      "source": [
        "#split the dataset into train and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, data['label'], test_size=0.15, random_state=111)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vnnkK2fT1fh"
      },
      "source": [
        "## **Classifying using sklearn pre built classifiers**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOPj1HQhT1fi"
      },
      "source": [
        "#import sklearn packages for building classifiers\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1CE9S30T1fl"
      },
      "source": [
        "#initialize multiple classification models \n",
        "svc = SVC(kernel='sigmoid', gamma=1.0)\n",
        "knc = KNeighborsClassifier(n_neighbors=49)\n",
        "mnb = MultinomialNB(alpha=0.2)\n",
        "dtc = DecisionTreeClassifier(min_samples_split=7, random_state=111)\n",
        "lrc = LogisticRegression(solver='liblinear', penalty='l1')\n",
        "rfc = RandomForestClassifier(n_estimators=31, random_state=111)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fP-v8pdvT1fo"
      },
      "source": [
        "#create a dictionary of variables and models\n",
        "clfs = {'SVC' : svc,'KN' : knc, 'NB': mnb, 'DT': dtc, 'LR': lrc, 'RF': rfc}"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WimrKW_XT1fv"
      },
      "source": [
        "#fit the data onto the models\n",
        "def train(clf, features, targets):    \n",
        "    clf.fit(features, targets)\n",
        "\n",
        "def predict(clf, features):\n",
        "    return (clf.predict(features))"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GKN_IUWT1fy"
      },
      "source": [
        "pred_scores_word_vectors = []\n",
        "for k,v in clfs.items():\n",
        "    train(v, X_train, y_train)\n",
        "    pred = predict(v, X_test)\n",
        "    pred_scores_word_vectors.append((k, [accuracy_score(y_test , pred)]))"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryfz7ed_T1f2"
      },
      "source": [
        "## **Predictions using TFIDF Vectorizer algorithm**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "qIRrQXPOT1f2",
        "outputId": "a5b34508-34d9-421a-963c-f20b5838607f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "pred_scores_word_vectors"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('SVC', [0.9892344497607656]),\n",
              " ('KN', [0.9617224880382775]),\n",
              " ('NB', [0.9940191387559809]),\n",
              " ('DT', [0.9677033492822966]),\n",
              " ('LR', [0.9688995215311005]),\n",
              " ('RF', [0.9856459330143541])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSExV8qIT1gC"
      },
      "source": [
        "##**Model predictions**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yz_DheJ1T1gN"
      },
      "source": [
        "#write functions to detect if the message is spam or not\n",
        "def find(x):\n",
        "    if x == 'spam':\n",
        "        print (\"Message is SPAM\")\n",
        "    else:\n",
        "        print (\"Message is NOT Spam\")"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5ytyDshT1gR"
      },
      "source": [
        "newtext = [\"win FREE mobiles and play games\"]\n",
        "integers = vectorizer.transform(newtext)"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imQD1ahzT1gT",
        "outputId": "ed529783-c2ce-4e1b-c304-7589d502a52f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x = mnb.predict(integers)\n",
        "find(x)        "
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Message is SPAM\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQ62GGOlFT0o"
      },
      "source": [
        ""
      ],
      "execution_count": 29,
      "outputs": []
    }
  ]
}